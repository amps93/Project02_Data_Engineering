{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e66e682-5246-477d-89b3-b53989c95419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "driver = webdriver.Chrome(r'./chromedriver.exe')\n",
    "driver.maximize_window() \n",
    "\n",
    "items = ['서울', '맛집']\n",
    "\n",
    "rating_df = []\n",
    "userid_list = []\n",
    "itemid_list = []\n",
    "timestamp_list = []\n",
    "comment_list = []\n",
    "\n",
    "count = 0\n",
    "current = 0\n",
    "goal = len(items)\n",
    "for item in items :\n",
    "    current += 1\n",
    "    print('진행상황 : ', current,'/',goal,sep=\"\")\n",
    "    # 리뷰가 없을 때의 코드\n",
    "    driver.get(\"https://map.kakao.com/\") # 카카오 지도 접속하기\n",
    "    searchbox = driver.find_element_by_xpath(\"//input[@id='search.keyword.query']\") # 검색창에 입력하기\n",
    "    searchbox.send_keys(item)\n",
    "    time.sleep(2)\n",
    "    searchbutton = driver.find_element_by_xpath(\"//button[@id='search.keyword.submit']\") # 검색버튼 누르기\n",
    "    driver.execute_script(\"arguments[0].click();\", searchbutton)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    if len(driver.find_elements_by_xpath(\"//a[@class='moreview']\")) != 0:\n",
    "        print('식당 존재')\n",
    "        driver.execute_script('window.open(\"about:blank\", \"_blank\");') # 새 탭 열기\n",
    "        reviewbutton = driver.find_element_by_xpath(\"//a[@class='numberofscore']\")\n",
    "        time.sleep(2)\n",
    "        content_url = reviewbutton.get_attribute(\"href\") \n",
    "        tabs = driver.window_handles\n",
    "        driver.switch_to_window(tabs[1]) # 새 탭으로 이동\n",
    "        driver.get(content_url) # 링크 열기\n",
    "        time.sleep(3)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        review_lists = soup.select('.list_evaluation > li')\n",
    "        print(len(review_lists))\n",
    "        if len(review_lists) != 0 :\n",
    "            for i, review in enumerate(review_lists) :\n",
    "                user_review = review.select('.txt_comment > span') # 리뷰\n",
    "                rating = review.select('.grade_star > em') # 별점\n",
    "                try:\n",
    "                    img_url = review_lists[i].select_one('a.link_photo > img ')['src']\n",
    "                except:\n",
    "                    continue\n",
    "                user_id = review.select('.append_item > a[data-userid]') # user 정보 html 파싱\n",
    "                timestamp = review.select(' div > span.time_write') #시간정보\n",
    "                try:\n",
    "                    row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\":item,\"Rating\":rating[0].text,\"Timestamp\":timestamp[0].text}\n",
    "                    row = pd.DataFrame(row, index=[i])\n",
    "                    rating_df = rating_df.append(row,ignore_index=True)\n",
    "                    review_row = {\"ItemID\" : item, \"review\" : user_review[0].text}\n",
    "                    review_row = pd.DataFrame(review_row, index=[i])\n",
    "                    review_elem = review_elem.append(review_row, ignore_index = True)\n",
    "                    try :\n",
    "                        img_row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\" : item, \"img_url\" : img_url}\n",
    "                        img_row = pd.DataFrame(img_row, index=[i])\n",
    "                        img_elem = img_elem.append(img_row, ignore_index=True)\n",
    "                    except :\n",
    "                        img_row = {\"UserID\":None,\"ItemID\" : item, \"img_url\" : None}\n",
    "                        img_row = pd.DataFrame(img_row, index=[i])\n",
    "                        img_elem = img_elem.append(img_row, ignore_index=True)\n",
    "\n",
    "                    time.sleep(3)\n",
    "                except:\n",
    "                    row = {\"UserID\":None,\"ItemID\":item,\"Rating\":None,\"Timestamp\":timestamp[0].text}\n",
    "                    row = pd.DataFrame(row, index=[i])\n",
    "                    rating_df = rating_df.append(row,ignore_index=True)\n",
    "                    review_row = {\"ItemID\" : item, \"review\" : user_review[0].text}\n",
    "                    review_row = pd.DataFrame(review_row, index=[i])\n",
    "                    review_elem = review_elem.append(review_row, ignore_index = True)\n",
    "                    try :\n",
    "                        img_row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\" : item, \"img_url\" : img_url}\n",
    "                        img_row = pd.DataFrame(img_row, index=[i])\n",
    "                        img_elem = img_elem.append(img_row, ignore_index=True)\n",
    "                    except :\n",
    "                        img_row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\" : item, \"img_url\" : None}\n",
    "                        img_row = pd.DataFrame(img_row, index=[i])\n",
    "                        img_elem = img_elem.append(img_row, ignore_index=True)\n",
    "                    time.sleep(1)\n",
    "\n",
    "                \n",
    "                    \n",
    "        else :\n",
    "            print(\"리뷰가 없습니다\")\n",
    "            \n",
    "        try:\n",
    "            for i in range(2,500):\n",
    "                time.sleep(3)\n",
    "                another_review = driver.find_element_by_xpath(\"//a[@data-page='\" + str(i) + \"']\")\n",
    "                another_review.click()\n",
    "                time.sleep(3)\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                review_lists = soup.select('.list_evaluation > li')\n",
    "                if len(review_lists) != 0 :\n",
    "                    for i, review in enumerate(review_lists) :\n",
    "                        user_review = review.select('.txt_comment > span') # 리뷰\n",
    "                        rating = review.select('.grade_star > em') # 별점\n",
    "                        try:\n",
    "                            img_url = review_lists[i].select_one('a.link_photo > img ')['src']\n",
    "                        except:\n",
    "                            continue\n",
    "                        user_id = review.select('.append_item > a[data-userid]') # user 정보 html 파싱\n",
    "                        timestamp = review.select(' div > span.time_write') #시간정보\n",
    "                        try:\n",
    "                            row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\":item,\"Rating\":rating[0].text,\"Timestamp\":timestamp[0].text}\n",
    "                            row = pd.DataFrame(row, index=[i])\n",
    "                            rating_df = rating_df.append(row,ignore_index=True)\n",
    "                            review_row = {\"UserID\":user_id[0].get('data-userid'), \"ItemID\" : item, \"review\" : user_review[0].text}\n",
    "                            review_row = pd.DataFrame(review_row, index=[i])\n",
    "                            review_elem = review_elem.append(review_row, ignore_index = True)\n",
    "                            try:\n",
    "                                img_row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\" : item, \"img_url\" : img_url}\n",
    "                                img_row = pd.DataFrame(img_row, index=[i])\n",
    "                                img_elem = img_elem.append(img_row, ignore_index=True)\n",
    "                            except:\n",
    "                                img_row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\" : item, \"img_url\" : None}\n",
    "                                img_row = pd.DataFrame(img_row, index=[i])\n",
    "                                img_elem = img_elem.append(img_row, ignore_index=True)\n",
    "                            time.sleep(1)\n",
    "                            \n",
    "                        except:\n",
    "                            row = {\"UserID\":None,\"ItemID\":item,\"Rating\":None,\"Timestamp\":timestamp[0].text}\n",
    "                            row = pd.DataFrame(row, index=[i])\n",
    "                            rating_df = rating_df.append(row,ignore_index=True)\n",
    "                            review_row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\" : item, \"review\" : user_review[0].text}\n",
    "                            review_row = pd.DataFrame(review_row, index=[i])\n",
    "                            review_elem = review_elem.append(review_row, ignore_index = True)\n",
    "                            try :\n",
    "                                img_row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\" : item, \"img_url\" : img_url}\n",
    "                                img_row = pd.DataFrame(img_row, index=[i])\n",
    "                                img_elem = img_elem.append(img_row, ignore_index=True)\n",
    "                            except :\n",
    "                                img_row = {\"UserID\":user_id[0].get('data-userid'),\"ItemID\" : item, \"img_url\" : None}\n",
    "                                img_row = pd.DataFrame(img_row, index=[i])\n",
    "                                img_elem = img_elem.append(img_row, ignore_index=True)\n",
    "                            \n",
    "                        \n",
    "                            \n",
    "        except:\n",
    "            print(\"더 이상 리뷰 존재 X\")\n",
    "            driver.close()\n",
    "        driver.switch_to_window(tabs[0])\n",
    "        print(\"기본 페이지로 돌아가자\")\n",
    "            \n",
    "    else:\n",
    "        print(\"식당 존재 x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa03e30f-f902-4183-954f-a3723225bfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import sleep\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "##############################################################  ############\n",
    "##################### variable related selenium ##########################\n",
    "##########################################################################\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "options.add_argument('lang=ko_KR')\n",
    "chromedriver_path = \"chromedriver\"\n",
    "driver = webdriver.Chrome(os.path.join(os.getcwd(), chromedriver_path), options=options)  # chromedriver 열기\n",
    "\n",
    "\n",
    "def main():\n",
    "    global driver, load_wb, review_num\n",
    "\n",
    "    driver.implicitly_wait(4)  # 렌더링 될때까지 기다린다 4초\n",
    "    driver.get('https://map.kakao.com/')  # 주소 가져오기\n",
    "\n",
    "    # 검색할 목록\n",
    "    place_infos = ['강남 맛집']\n",
    "\n",
    "    for i, place in enumerate(place_infos):\n",
    "        # delay\n",
    "        if i % 4 == 0 and i != 0:\n",
    "            sleep(5)\n",
    "        print(\"#####\", i)\n",
    "        search(place)\n",
    "\n",
    "    driver.quit()\n",
    "    print(\"finish\")\n",
    "\n",
    "\n",
    "def search(place):\n",
    "    global driver\n",
    "\n",
    "    search_area = driver.find_element_by_xpath('//*[@id=\"search.keyword.query\"]')  # 검색 창\n",
    "    search_area.send_keys(place)  # 검색어 입력\n",
    "    driver.find_element_by_xpath('//*[@id=\"search.keyword.submit\"]').send_keys(Keys.ENTER)  # Enter로 검색\n",
    "    sleep(1)\n",
    "\n",
    "    # 검색된 정보가 있는 경우에만 탐색\n",
    "    # 1번 페이지 place list 읽기\n",
    "    html = driver.page_source\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    place_lists = soup.select('.placelist > .PlaceItem') # 검색된 장소 목록\n",
    "\n",
    "    # 검색된 첫 페이지 장소 목록 크롤링하기\n",
    "    crawling(place, place_lists)\n",
    "    search_area.clear()\n",
    "\n",
    "    # 우선 더보기 클릭해서 2페이지\n",
    "    try:\n",
    "        driver.find_element_by_xpath('//*[@id=\"info.search.place.more\"]').send_keys(Keys.ENTER)\n",
    "        sleep(1)\n",
    "\n",
    "        # 2~ 5페이지 읽기\n",
    "        for i in range(2, 6):\n",
    "            # 페이지 넘기기\n",
    "            xPath = '//*[@id=\"info.search.page.no' + str(i) + '\"]'\n",
    "            driver.find_element_by_xpath(xPath).send_keys(Keys.ENTER)\n",
    "            sleep(1)\n",
    "\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            place_lists = soup.select('.placelist > .PlaceItem') # 장소 목록 list\n",
    "\n",
    "            crawling(place, place_lists)\n",
    "\n",
    "    except ElementNotInteractableException:\n",
    "        print('not found')\n",
    "    finally:\n",
    "        search_area.clear()\n",
    "\n",
    "\n",
    "def crawling(place, place_lists):\n",
    "    \"\"\"\n",
    "    페이지 목록을 받아서 크롤링 하는 함수\n",
    "    :param place: 리뷰 정보 찾을 장소이름\n",
    "    \"\"\"\n",
    "\n",
    "    while_flag = False\n",
    "    for i, place in enumerate(place_lists):\n",
    "        # 광고에 따라서 index 조정해야함\n",
    "        #if i >= 3:\n",
    "         #   i += 1\n",
    "\n",
    "        place_name = place.select('.head_item > .tit_name > .link_name')[0].text  # place name\n",
    "        place_address = place.select('.info_item > .addr > p')[0].text  # place address\n",
    "\n",
    "        detail_page_xpath = '//*[@id=\"info.search.place.list\"]/li[' + str(i + 1) + ']/div[5]/div[4]/a[1]'\n",
    "        driver.find_element_by_xpath(detail_page_xpath).send_keys(Keys.ENTER)\n",
    "        driver.switch_to.window(driver.window_handles[-1])  # 상세정보 탭으로 변환\n",
    "        sleep(1)\n",
    "\n",
    "        print('####', place_name)\n",
    "\n",
    "        # 첫 페이지\n",
    "        extract_review(place_name)\n",
    "\n",
    "        # 2-5 페이지\n",
    "        idx = 3\n",
    "        try:\n",
    "            page_num = len(driver.find_elements_by_class_name('link_page')) # 페이지 수 찾기\n",
    "            for i in range(page_num-1):\n",
    "                # css selector를 이용해 페이지 버튼 누르기\n",
    "                driver.find_element_by_css_selector('#mArticle > div.cont_evaluation > div.evaluation_review > div > a:nth-child(' + str(idx) +')').send_keys(Keys.ENTER)\n",
    "                sleep(1)\n",
    "                extract_review(place_name)\n",
    "                idx += 1\n",
    "            driver.find_element_by_link_text('다음').send_keys(Keys.ENTER) # 5페이지가 넘는 경우 다음 버튼 누르기\n",
    "            sleep(1)\n",
    "            extract_review(place_name) # 리뷰 추출\n",
    "        except (NoSuchElementException, ElementNotInteractableException):\n",
    "            print(\"no review in crawling\")\n",
    "\n",
    "        # 그 이후 페이지\n",
    "        while True:\n",
    "            idx = 4\n",
    "            try:\n",
    "                page_num = len(driver.find_elements_by_class_name('link_page'))\n",
    "                for i in range(page_num-1):\n",
    "                    driver.find_element_by_css_selector('#mArticle > div.cont_evaluation > div.evaluation_review > div > a:nth-child(' + str(idx) +')').send_keys(Keys.ENTER)\n",
    "                    sleep(1)\n",
    "                    extract_review(place_name)\n",
    "                    idx += 1\n",
    "                driver.find_element_by_link_text('다음').send_keys(Keys.ENTER) # 10페이지 이상으로 넘어가기 위한 다음 버튼 클릭\n",
    "                sleep(1)\n",
    "                extract_review(place_name) # 리뷰 추출\n",
    "            except (NoSuchElementException, ElementNotInteractableException):\n",
    "                print(\"no review in crawling\")\n",
    "                break\n",
    "\n",
    "        driver.close()\n",
    "        driver.switch_to.window(driver.window_handles[0])  # 검색 탭으로 전환\n",
    "\n",
    "\n",
    "def extract_review(place_name):\n",
    "    global driver\n",
    "\n",
    "    ret = True\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # 첫 페이지 리뷰 목록 찾기\n",
    "    review_lists = soup.select('.list_evaluation > li')\n",
    "\n",
    "    # 리뷰가 있는 경우\n",
    "    if len(review_lists) != 0:\n",
    "        for i, review in enumerate(review_lists):\n",
    "            comment = review.select('.txt_comment > span') # 리뷰\n",
    "            rating = review.select('.grade_star > em') # 별점\n",
    "            val = ''\n",
    "            if len(comment) != 0:\n",
    "                if len(rating) != 0:\n",
    "                    val = comment[0].text + '/' + rating[0].text.replace('점', '')\n",
    "                else:\n",
    "                    val = comment[0].text + '/0'\n",
    "                print(val)\n",
    "\n",
    "    else:\n",
    "        print('no review in extract')\n",
    "        ret = False\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4b791f1-174a-4154-bfc9-69b81f127cc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-c0cab1d8b126>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# 네이버 지도 검색창에 [~동 @@식당]으로 검색해 정확도를 높여야 합니다. 검색어를 미리 설정해줍시다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'naver_keyword'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'정자동'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"%20\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'코이라멘'\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# \"%20\"는 띄어쓰기를 의미합니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'naver_map_url'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://map.naver.com/v5'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(r'./chromedriver.exe')\n",
    "\n",
    "# 포스팅 작성 당시 크롬 버젼 : 92\n",
    "df = pd.read_csv('shops.csv', sep='|') \n",
    "\n",
    "# 네이버 지도 검색창에 [~동 @@식당]으로 검색해 정확도를 높여야 합니다. 검색어를 미리 설정해줍시다.\n",
    "\n",
    "df['naver_keyword'] = df['정자동'] + \"%20\" + df['코이라멘']  # \"%20\"는 띄어쓰기를 의미합니다.\n",
    "df['naver_map_url'] = 'https://map.naver.com/v5'\n",
    "\n",
    "\n",
    "# 본격적으로 가게 상세페이지의 URL을 가져옵시다\n",
    "\n",
    "for i, keyword in enumerate(df['naver_keyword'].tolist()):\n",
    "    print(\"이번에 찾을 키워드 :\", i, f\"/ {df.shape[0] -1} 행\", keyword)\n",
    "    try:\n",
    "        naver_map_search_url = f\"https://m.map.naver.com/search2/search.naver?query={keyword}&sm=hty&style=v5\"\n",
    "        \n",
    "        driver.get(naver_map_search_url)\n",
    "        time.sleep(3.5)\n",
    "        df.iloc[i,-1] = driver.find_element_by_css_selector(\"#ct > div.search_listview._content._ctList > ul > li:nth-child(1) > div.item_info > a.a_item.a_item_distance._linkSiteview\").get_attribute('data-cid')\n",
    "        # 네이버 지도 시스템은 data-cid에 url 파라미터를 저장해두고 있었습니다.\n",
    "        # data-cid 번호를 뽑아두었다가 기본 url 템플릿에 넣어 최종적인 url을 완성하면 됩니다.\n",
    "        \n",
    "        #만약 검색 결과가 없다면?\n",
    "    except Exception as e1:\n",
    "        if \"li:nth-child(1)\" in str(e1):  # -> \"child(1)이 없던데요?\"\n",
    "            try:\n",
    "                df.iloc[i,-1] = driver.find_element_by_css_selector(\"#ct > div.search_listview._content._ctList > ul > li:nth-child(1) > div.item_info > a.a_item.a_item_distance._linkSiteview\").get_attribute('data-cid')\n",
    "                time.sleep(1)\n",
    "            except Exception as e2:\n",
    "                print(e2)\n",
    "                df.iloc[i,-1] = np.nan\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "# 이때 수집한 것은 완전한 URL이 아니라 URL에 들어갈 ID (data-cid 라는 코드명으로 저장된) 이므로, 온전한 URL로 만들어줍니다\n",
    "\n",
    "df['naver_map_url'] = \"https://m.place.naver.com/restaurant/\" + df['naver_map_url']\n",
    "\n",
    "\n",
    "# URL이 수집되지 않은 데이터는 제거합니다.\n",
    "df = df.loc[~df['naver_map_url'].isnull()]\n",
    "\n",
    "# 크롤링 에러가 떠서 'null'을 넣어 둔 데이터는 활용 의미가 없으므로 행 삭제를 해줘도 됩니다\n",
    "df = df.loc[~(df['naver_store_type'].str.contains('null'))]\n",
    "\n",
    "# 별점 평균, 수 같은 데이터 역시 스트링 타입으로 크롤링이 되었으므로 numeric으로 바꿔줍니다.\n",
    "df[['naver_star_point', 'naver_star_point_qty', 'naver_blog_review_qty']] = df[['naver_star_point', 'naver_star_point_qty', 'naver_blog_review_qty']].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac03793-e847-4af3-b668-6d6812c39582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Multi",
   "language": "python",
   "name": "multi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
