{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d013d25a-5160-4e23-9409-932a79de2ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### 0 : 금천구 맛집\n",
      "#### 광고만두라\n",
      "ㅜㅜ 방문했는데 아직 안열었네요ㅠㅠ 맛보고싶었는데ㅜ/4\n",
      "주변 상인들 말로는 6월달부터 영업하신다고 전달받았는데 아직까지도 영업하지 않으심.. 5월 중순부터 3번은 찾아가본 듯 한데 언제쯤 먹어볼 수 있을지…/3\n",
      "못먹어봐서 ㅠ 6월10일 어제 갔는데 개인 사정으로 당분간 쉰다고 적혀 있었어요 ㅠ 언제까지 쉬는지는 잘 모름 안적혀 있었음./3\n",
      "만두로는 최고/5\n",
      "금천구 맛집 찾다가 들렸는데 김치만두 최고네요~~ 속도 꽉차고 3개 먹으니 너무 배불르네요/5\n",
      "/5\n",
      "우연히 예전 방송에 나온거 보고 갑자기 먹고 싶어서 방문했는데 동네 만두집하고 큰 차이는 모르겠어요 ㅠㅠ/3\n",
      "통만두 맛집/5\n",
      "만두 맛집이라고해서 따라가서 먹은 집. 김치만두만 먹어서 김치만두국 먹었는데, 김치만두는 정말 별로였음../2\n",
      "맛은 그럭저럭 평범하고 불친절해요/2\n",
      "공구단지내 종사자와 방문객들의 오랜 사랑을 받고 있는 집으로 속이 비치는 육향가득 통만두가 맛이 좋다./4\n",
      "통만두는 맛있지만 김치만두는 최악.만두국도 맛없고 김치만두가 맛있어야 진짜 잘하는집인데, 일반 시장통 만두집에 비해 별차이없는 흔한맛에 공구상가 월세가 비쌀리도 없는데 모든 메뉴가 비싼편이고 만두는 3500원, 만두국은 6천원 받아야 적당한 가격임. /1\n",
      "김치통만두랑 만둣국 시켰음 매장안은 깔끔하고 만두도 금방 나옴 매장분들 모두 정말 친절하심 김치만두는 평소에 비려서 많이 안좋아하는데 굉장히 부드럽고 공장만두보다 간이 삼삼해서 좋음 만둣국은 사골국물에 물만두가 나오는데  정말정말 부드러워서 후루룩 마셔먹을수 있음 만족만족 대만족 재방문 의사 있음/5\n",
      "만두피가 얇고 부드러워요. 만두 속도 잘 다져져있어서 피랑 굉장히 잘어울려요. 만둣국이랑 김치만두 먹었는데 둘다 최고!! 만둣국은 사골국물이 진하고 김치만두는 속이 비리지않고 김치맛이 적당히 나서 좋았어요ㅠㅠ 가격도 저렴하고 서비스도 훌륭해요. 만둣국 시키면 밥을 주시는데 리필 가능한 듯 해요!! 많이들 가셨으면 좋겠습니다:-D/5\n",
      "/3\n",
      "/5\n",
      "부들부들한 만두피에 한 입에 먹기 좋은 만두가 가득 들어있는 사골만두국이 이 집의 Top. 더 달라하면 얼마든지 내주는 밥과 겉절이김치까지 먹을 수 있다. 주인 내외분들도 친절하시고, 가게도 깨끗해서 다녀오면 기분까지 좋아지는 밥집이다. 점심 피크타임을 비켜 가면 주차도 한결 수월하다./5\n",
      "정직한맛. 굳이 단점을 찾자면 주차장때문에 가기 꺼려진다../5\n",
      "맛나네용.  점심 먹으러 들렀는데 역시 육수가 좋아요/4\n",
      "맛있어요/5\n",
      "/5\n",
      "시흥3동 954-21 공구상가4번건물 2번출입구/3\n",
      "맛으로는 8점, 친절과 위생, 가격을 더한다면 10점 만점... 기회(?)가 된다면 다시 가볼 것 같다.../5\n",
      "/4\n",
      "/5\n",
      "생활의 달인에 이어 수요미식회 만두 맛집~ 만두국이 특히 맛있었어요^^/4\n",
      "\"HOLY Mandu\"  전세계 돌아 다녀도 이정도 가격에 이정도 가성비 찾기 힘듬.  상황상 군만두를 과감히 뺀거 보면 만두 아시는 분임.  다만 알바들이 문제....서비스 해본적 없는...분들 쓰시는듯...  단 메뉴 볼것 없고 통만두 2인분 모밀국수 1인분..만육천원으로 행복한 오후 보낸다.  부대시설 인테리어 화장실 이런건 안드로메다로 보내고 가야 .../5\n",
      "/3\n",
      "꼭  방문하겠습니다/5\n",
      "no review in crawling\n",
      "#### 가산물갈비&백년불고기\n",
      "/5\n",
      "회식 장소로는 갸우뚱. 하지만 동성 친구끼리 가성비 있게 먹을 장소로 제격입니다. 양 많고 맛 좋고 서비스 괜찮아요./5\n",
      "여기가 맛있어요? 회식장소로 좋아요? 아 내돈 내산은 안좋다는거죠 갔을때 먹고, 아 여긴 두번다시 안올맛이다. 싶어요 한약재로 만들었다고 하는데,그게 진짜 사족의맛 이에요./1\n",
      "회식장소로 좋구요 오징어튀김이 별미예요 엄청 바삭해요 /5\n",
      "맛있음/5\n",
      "회식으로는 딱입니다/5\n",
      "맛있어요 양도많고ㅋ/4\n",
      "맵찔이는 먹기 힘들어요! 그러나 오징어튀김 있으니까 괜찮아요 ㅋㅋㅋ 맛있음다/4\n",
      "/5\n",
      "양도 많고 맛있어요/4\n",
      "맛있는데 양이 진짜 너무많아서 4분이시면 3인분시키고 오징어튀김시키세요 오징어튀김 맛있어요! 인기가 많아서그런지 뭐.. 친절이나 서비스는 그저그렇습니다/5\n",
      "칼칼하고 국물에 소고기면 정답인듯/5\n",
      "맛있대서 갔는데 있던 입맛도 떨어지는 맛임 달고 짜고 채소 산더미 쌓아주고 고기조금올리고/1\n",
      "외형 : 위에 올라간 샤브샤브 고기와 아래 깔린 갈비, 버섯 두조각에 많은 콩나물. 양은 가격대비 적당.  식평 : 샤브고기 질 괜찮고 하단 갈비살은 비린향을 못잡음. 무엇보다 기본 육수가 지나치게 달아요. 육수맛때문에 다먹지도 못하고 남겼어요. 물려요. 회사건물에 광고 정말많이나오길래 가봤는데 기대이하.  어르신들은 음식이 단거 싫어하므로 가족외식용으로는.../2\n",
      "/4\n",
      "/4\n",
      "술안주로 좋다. 볶음밥이 아쉽다./4\n",
      "맛있습니다/5\n",
      "음 여기왜 점수 3.8점인지 모르겠네요. 고기는 평범한 약간 누린내가 있구요 국물은 달아요. 떡볶이맛이 나요.... 어른들 모시고 맛집일거라고 생각해서 왔는데. 실망이네요. 양은 많습니다. 2.5점이 적당해요. 그나마 제일맛있는건 동치미입니다. /2\n",
      "광고보구갔다가 실망/2\n",
      "불고기 짱~!/5\n",
      "/4\n",
      "국물이 달고 고기에서 아주약간 누린내가 난다. 가격도 비싸고… 어른들 모시고 갔다가 민망했음 ㅠㅠ/1\n",
      "점심먹으러갔는데 점심정식주문하니 다른비싼메뉴 소불고기 먹으라고함/1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "##############################################################  ############\n",
    "##################### variable related selenium ##########################\n",
    "##########################################################################\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "options.add_argument('lang=ko_KR')\n",
    "\n",
    "driver = webdriver.Chrome('./chromedriver')\n",
    "\n",
    "rating_df = pd.DataFrame()\n",
    "restaurant_df = pd.DataFrame()\n",
    "\n",
    "def main():\n",
    "    global driver, load_wb, review_num, rating_df, restaurant_df\n",
    "\n",
    "    driver.implicitly_wait(4)  # 렌더링 될때까지 기다린다 4초\n",
    "    driver.get('https://map.kakao.com/')  # 주소 가져오기\n",
    "\n",
    "     # 검색할 목록\n",
    "#     place_infos = ['종로구 맛집', '중구 맛집', ' 용산구 맛집', '성동구 맛집', '광진구 맛집', '동대문구 맛집', '중랑구 맛집', '성북구 맛집', '강북구 맛집']\n",
    "#     place_infos = ['도봉구 맛집', '노원구 맛집', '은평구 맛집', '서대문구 맛집', '마포구 맛집', '양천구 맛집', '강서구 맛집','구로구 맛집']\n",
    "    place_infos = ['금천구 맛집', '영등포구 맛집', '동작구 맛집', '관악구 맛집', '서초구 맛집', '강남구 맛집', '송파구 맛집','강동구 맛집']\n",
    "    \n",
    "    for i, place in enumerate(place_infos):\n",
    "        print(\"#####\", i,\":\",place)\n",
    "        search(place)\n",
    "        \n",
    "        rating_df.to_csv('%s_rating_df.csv' %place)\n",
    "        rating_df.to_csv('%s_rating_df_ko.csv' %place, sep=',', na_rep='NaN', encoding='utf-8-sig')\n",
    "        \n",
    "        restaurant_df.to_csv('%s_restaurant_df.csv'  %place)\n",
    "        restaurant_df.to_csv('%s_restaurant_df_ko.csv'  %place, sep=',', na_rep='NaN', encoding='utf-8-sig')\n",
    "        \n",
    "    driver.quit()\n",
    "    print(\"finish\")\n",
    "\n",
    "\n",
    "def search(place):\n",
    "    global driver\n",
    "\n",
    "    search_area = driver.find_element_by_xpath('//*[@id=\"search.keyword.query\"]')  # 검색 창\n",
    "    search_area.send_keys(place)  # 검색어 입력\n",
    "    driver.find_element_by_xpath('//*[@id=\"search.keyword.submit\"]').send_keys(Keys.ENTER)  # Enter로 검색\n",
    "    driver.find_element_by_xpath('//*[@id=\"info.search.place.more\"]').send_keys(Keys.ENTER) # 더보기\n",
    "    xPath = '//*[@id=\"info.search.page.no1\"]'\n",
    "    driver.find_element_by_xpath(xPath).send_keys(Keys.ENTER)\n",
    "    sleep(1)\n",
    "\n",
    "    # 검색된 정보가 있는 경우에만 탐색\n",
    "    # 1번 페이지 place list 읽기\n",
    "    html = driver.page_source\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    place_lists = soup.select('.placelist > .PlaceItem') # 검색된 장소 목록\n",
    "\n",
    "    # 검색된 첫 페이지 장소 목록 크롤링하기\n",
    "    crawling(place, place_lists)\n",
    "    search_area.clear()\n",
    "\n",
    "    # 전체 페이지\n",
    "    while True:\n",
    "        try:\n",
    "            # 2~ 5페이지 읽기\n",
    "            for i in range(2, 6):\n",
    "                # 페이지 넘기기\n",
    "                xPath = '//*[@id=\"info.search.page.no' + str(i) + '\"]'\n",
    "                driver.find_element_by_xpath(xPath).send_keys(Keys.ENTER)\n",
    "                sleep(1)\n",
    "\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                place_lists = soup.select('.placelist > .PlaceItem') # 장소 목록 list\n",
    "                \n",
    "                crawling(place, place_lists)\n",
    "                \n",
    "                # 다음 페이지 넘기기\n",
    "                if i==5:\n",
    "                    driver.find_element_by_xpath('//*[@id=\"info.search.page.next\"]').send_keys(Keys.ENTER)\n",
    "\n",
    "        except ElementNotInteractableException:\n",
    "            print('end page')\n",
    "            break\n",
    "#         finally:\n",
    "#             search_area.clear()\n",
    "\n",
    "\n",
    "def crawling(place, place_lists):\n",
    "    \"\"\"\n",
    "    페이지 목록을 받아서 크롤링 하는 함수\n",
    "    :param place: 리뷰 정보 찾을 장소이름\n",
    "    \"\"\"\n",
    "    \n",
    "    global restaurant_df\n",
    "\n",
    "    while_flag = False\n",
    "    for i, place in enumerate(place_lists):\n",
    "\n",
    "        place_name = place.select('.head_item > .tit_name > .link_name')[0].text  # place name\n",
    "        place_address = place.select('.info_item > .addr > p')[0].text  # place address\n",
    "        place_local = place.select('.info_item > .addr > .lot_number')[0].text\n",
    "        place_category = place.select('.head_item > .subcategory')[0].text\n",
    "        place_detail = place.select('.info_item > .contact> .moreview')[0].get('href') # place detail\n",
    "        \n",
    "        row = {\"ItemID\":place_name, \"address\": place_address, \"local\" : place_local, \"category\": place_category}\n",
    "        row = pd.DataFrame(row, index=[1])\n",
    "        restaurant_df = restaurant_df.append(row, ignore_index=True)\n",
    "        \n",
    "        driver.execute_script('window.open(\"about:blank\", \"_blank\");')\n",
    "        driver.switch_to.window(driver.window_handles[-1])\n",
    "        driver.get(place_detail) # 상세정보 탭으로 변환\n",
    "        sleep(1)\n",
    "        \n",
    "        print('####', place_name)\n",
    "\n",
    "        # 첫 페이지\n",
    "        extract_review(place_name) # 리뷰 추출\n",
    "\n",
    "        # 2-5 페이지\n",
    "        idx = 3\n",
    "        try:\n",
    "            page_num = len(driver.find_elements_by_class_name('link_page')) # 페이지 수 찾기\n",
    "            for i in range(page_num-1):\n",
    "                # css selector를 이용해 페이지 버튼 누르기\n",
    "                driver.find_element_by_css_selector('#mArticle > div.cont_evaluation > div.evaluation_review > div > a:nth-child(' + str(idx) +')').send_keys(Keys.ENTER)\n",
    "                sleep(1)\n",
    "                extract_review(place_name)\n",
    "                idx += 1\n",
    "            driver.find_element_by_link_text('다음').send_keys(Keys.ENTER) # 5페이지가 넘는 경우 다음 버튼 누르기\n",
    "            \n",
    "            sleep(1)\n",
    "            extract_review(place_name) # 리뷰 추출\n",
    "            \n",
    "            # 그 이후 페이지\n",
    "            while True:\n",
    "                idx = 4\n",
    "                page_num = len(driver.find_elements_by_class_name('link_page')) #페이지 수 찾기\n",
    "                for i in range(page_num-1):\n",
    "                    driver.find_element_by_css_selector('#mArticle > div.cont_evaluation > div.evaluation_review > div > a:nth-child(' + str(idx) +')').send_keys(Keys.ENTER)\n",
    "                    sleep(1)\n",
    "                    extract_review(place_name)\n",
    "                    idx += 1\n",
    "                driver.find_element_by_link_text('다음').send_keys(Keys.ENTER) # 10페이지 이상으로 넘어가기 위한 다음 버튼 클릭\n",
    "                sleep(1)\n",
    "                extract_review(place_name) # 리뷰 추출            \n",
    "            \n",
    "        except (NoSuchElementException, ElementNotInteractableException):\n",
    "            print(\"no review in crawling\")\n",
    "\n",
    "        driver.close()\n",
    "        driver.switch_to.window(driver.window_handles[0])  # 검색 탭으로 전환\n",
    "\n",
    "\n",
    "def extract_review(place_name):\n",
    "    global driver, rating_df\n",
    "\n",
    "    ret = True\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # 첫 페이지 리뷰 목록 찾기\n",
    "    review_lists = soup.select('.list_evaluation > li')\n",
    "\n",
    "    # 리뷰가 있는 경우\n",
    "    if len(review_lists) != 0:\n",
    "        for i, review in enumerate(review_lists):\n",
    "            comment = review.select('.txt_comment > span') # 리뷰\n",
    "            rating = review.select('.grade_star > em') # 별점\n",
    "            user_id = review.select('.append_item > a[data-userid]') # user 정보 html 파싱\n",
    "            timestamp = review.select(' div > span.time_write') #시간정보\n",
    "            \n",
    "            val = ''\n",
    "            if len(comment) != 0:\n",
    "                if len(rating) != 0:\n",
    "                    val = comment[0].text + '/' + rating[0].text.replace('점', '')\n",
    "                else:\n",
    "                    val = comment[0].text + '/0'\n",
    "#                 print(val)                      \n",
    "                \n",
    "                try:\n",
    "                    row = {\"ItemID\":place_name, \"UserID\":user_id[0].get('data-userid'), \"review\" : comment[0].text,\n",
    "                           \"Rating\":rating[0].text.replace('점', ''), \"Timestamp\":timestamp[0].text}\n",
    "                    row = pd.DataFrame(row, index=[i])\n",
    "                    rating_df = rating_df.append(row, ignore_index=True)\n",
    "            \n",
    "                except:\n",
    "                    row = {\"ItemID\":place_name, \"UserID\":None, \"review\" : None,\n",
    "                           \"Rating\": None, \"Timestamp\":timestamp[0].text}\n",
    "                    row = pd.DataFrame(row, index=[i])\n",
    "                    rating_df = rating_df.append(row,ignore_index=True)\n",
    "                \n",
    "                \n",
    "    else:\n",
    "        print('no review in extract')\n",
    "        ret = False\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75993ed-3284-47a6-9519-3c36ff2614f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Multi",
   "language": "python",
   "name": "multi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
